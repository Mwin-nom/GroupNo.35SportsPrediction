# -*- coding: utf-8 -*-
"""Copy of Sports prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-zl0taNsZRBbmUBS_2FO7H6NzNWQY56v
"""

import pandas as pd
import os
import sklearn
import numpy as np
import pandas as pd
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn import tree, metrics
from sklearn.model_selection import train_test_split
from google.colab import drive
from sklearn.preprocessing import LabelEncoder


drive.mount('/content/drive')

"""## **Question 1**

## **Demonstrating the data preparation & feature extraction process**
"""

df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21.csv')

df2=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22.csv')

df.head()

df.info()

df.describe()

df.columns.tolist()

"""## **Removing useless varaiables**"""

df.drop(columns=['player_url','player_face_url','club_logo_url','club_flag_url','nation_flag_url'], inplace = True)

threshold = 0.3 * len(df)  # 30% threshold
df_no_useless_variables = df.dropna(axis=1, thresh=threshold)# dropping columns with more than 30% of their values missing

df_no_useless_variables.columns.tolist()

df_no_useless_variables.shape

df = df_no_useless_variables

"""## **Imputation for numerical variables using KNN Imputer**"""

df.dtypes.tolist() # trying to see the different datatypes in the dataframe

from pandas.core.arrays import categorical
numericVariables = df.select_dtypes(include=['int64','float64'])
categoricalVariables = df.select_dtypes(include=['object']) # separating the dataframe into numeric and categorical variables

from sklearn.impute import KNNImputer

knn_imputer = KNNImputer(n_neighbors=5)
numericVariables_imputed = knn_imputer.fit_transform(numericVariables)

numericVariables_imputed_df = pd.DataFrame(numericVariables_imputed,columns=numericVariables.columns)

numericVariables_imputed_df

numericVariables_imputed_df.isnull().sum().tolist() # checking to see if there are any null calues still

"""# **Imputation for categorical variables using Forward Filling**"""

categoricalVariables_imputed = categoricalVariables.ffill()

categoricalVariables_imputed_df = pd.DataFrame(categoricalVariables_imputed, columns =categoricalVariables.columns)

categoricalVariables_imputed_df

categoricalVariables_imputed_df.isnull().sum().tolist() # checking to see if there are any null calues still

"""## **Encoding the categorical variables using Onehot encoding**"""

#label_encoder = LabelEncoder()

#categoricalVariables_df_encoded = categoricalVariables_imputed_df.apply(label_encoder.fit_transform)

categoricalVariables_df_encoded = pd.get_dummies(categoricalVariables_imputed_df, prefix='categorical')

categoricalVariables_df_encoded

"""## **Creating a dataframe that contains both the numeric and categorical variables**"""

new_df = pd.concat([numericVariables_imputed_df, categoricalVariables_df_encoded], axis=1)

new_df

"""## **Separating the independent and dependent variables**"""

y = new_df['overall']
X = new_df.drop('overall', axis = 1)

"""## **Question 2**

# **Creating feature subsets which show better correlation with the overall rating and scaled the independent variables.**

**Created feature subsets which show better correlation with the overall rating**
"""

X_df =  pd.DataFrame(X)

correlations = X_df.corrwith(y)

correlations

selected_features = correlations[(correlations >= 0.4) | (correlations <= -0.4)] # selecting features with correlations greater than or
                                                                                 # equal to 0.4 and less than or equal to -0.4

selected_features.sort_values(ascending=False) # sorting the selected features in ascending order

feature_subset = X_df[selected_features.index]
X = feature_subset # putting the selected features in X

X_df

X.info()

"""## **Scaling the independent variables**"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

scaled=sc.fit_transform(X)

scaled.tolist()

from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42)
Xtrain.shape

"""## **Question 3**

# **Cross Validation for the Random Forest Regressor**
"""

from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

cv = KFold(n_splits=3)

RFM_PARAMETERS = {"max_depth":[2,5, 6, None],
              "max_features":[1.0,0.3,15],
              "min_samples_split": [2, 5, 10],
              "n_estimators":[100,500,300]}

full = RandomForestRegressor(n_jobs= -1)
RFM_model_gs = GridSearchCV (full, param_grid= RFM_PARAMETERS, cv=cv, scoring= "accuracy")
RFM_model_gs.fit ( Xtrain, Ytrain)
RFMy_pred= RFM_model_gs.predict(Xtest)

RFMy_pred

"""## **Cross Validation for the XGBoost Regressor**"""

import xgboost as xgb
from sklearn.model_selection import GridSearchCV
cv = KFold(n_splits=3)
PARAMETERS = {"subsample":[0.5, 0.75, 1],
              "max_depth":[2,5, 6, 12],
              "learning_rate":[0.3, 0.1, 0.03],
              "n_estimators":[100,500,1000]}

XGBModel = xgb.XGBRegressor(n_jobs= -1)

XGBmodel_gs = GridSearchCV(XGBModel, param_grid=PARAMETERS, cv=cv, scoring='neg_mean_squared_error')
XGBmodel_gs.fit(Xtrain,Ytrain)
XGBy_pred= XGBmodel_gs.predict(Xtest)

"""## **Cross Validation for the Gradient Boosting Regressor**"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
import numpy as np


model = GradientBoostingRegressor()

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]}

GBgrid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
GBgrid_search.fit(Xtrain, Ytrain)
GBy_pred = GBgrid_search.predict(Xtest)

"""## **Question 4**"""

from sklearn.metrics import mean_absolute_error

"""# Calculating the MAE of the RandomForestRegressor"""

mae_RFM = mean_absolute_error(Ytest, RFMy_pred )
print(f"Mean Absolute Error (MAE): {mae_RFM}")

"""# Calculating the MAE of the XGBoostRegressor"""

mae_XGB= mean_absolute_error(Ytest, XGBy_pred )
print(f"Mean Absolute Error (MAE): {mae_XGB}")

"""# Calculating the MAE of the GradientBoostingRegressor"""

mae_GB= mean_absolute_error(Ytest, GBy_pred )
print(f"Mean Absolute Error (MAE): {mae_GB}")

"""## **Question 5**"""

from pandas.core.arrays import categorical
numeric_TestVariables = df.select_dtypes(include=['int64','float64'])
categorical_TestVariables = df.select_dtypes(include=['object'])

"""## **Numeric** **Imputation for the 2022 dataset variables using KNN Imputation**"""

numeric_TestVariables_imputed = knn_imputer.fit_transform(numeric_TestVariables)

numeric_TestVariables_imputed_df = pd.DataFrame(numeric_TestVariables_imputed,columns=numeric_TestVariables.columns)

"""## **Imputation for the Categorical Variables of the 2022 dataset**"""

categorical_TestVariables_imputed = categorical_TestVariables.ffill()

categorical_TestVariables_imputed_df = pd.DataFrame(categorical_TestVariables_imputed, columns =categorical_TestVariables.columns)

"""## **Encoding the Categorical variables of the 2022 dataset**"""

categorical_TestVariables_df_encoded = pd.get_dummies(categorical_TestVariables_imputed_df, prefix='categorical')

new_df2 = pd.concat([numeric_TestVariables_imputed_df, categorical_TestVariables_df_encoded], axis=1)

"""## **Separating the independent and dependent variables in the 2022 dataset**"""

y2 = new_df2['overall']

new_df2

testData =new_df2[selected_features.index] # Extracting features with high correlation from the 2022 dataset based on observations in the 2021 dataset.
#Extracting the corresponding features from the 2022 dataset in the 2021 dataset and putting them in a dataframe

testData.info()

"""## **Scaling the independent variables of the 2022 dataset**"""

scaled2=sc.fit_transform(testData)

"""## **Testing the choosen model using the choosen columns from the 2022 dataset**"""

XGBy_pred2= XGBmodel_gs.predict(testData)

"""## **Calulating the Mean Absolute Error**"""

mae_XGB= mean_absolute_error(y2, XGBy_pred2 )
print(f"Mean Absolute Error (MAE): {mae_XGB}")

"""# **Saving the model in a pickle file**"""

import pickle
XGBmodel_gs_model = XGBmodel_gs

# Specify the filename for the pickle file
pickle_filename = 'XGBmodel_gs_model.pkl'

# Save the model to a pickle file
with open(pickle_filename, 'wb') as file:
    pickle.dump(XGBmodel_gs_model, file)





